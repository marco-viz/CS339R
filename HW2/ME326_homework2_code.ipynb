{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ati2qIDpxPz"
      },
      "source": [
        "# Homework 2\n",
        "## ME 326: Collaborative Robotics\n",
        "### Stanford University"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-speech\n",
        "!pip install google-cloud-vision\n",
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "E7Z2OGUEBNqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70044b50-7623-4b3b-e103-3d7c8884c8a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-speech in /usr/local/lib/python3.12/dist-packages (2.36.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.29.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech) (2.43.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech) (1.27.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-speech) (5.29.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.32.4)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-cloud-speech) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2026.1.4)\n",
            "Collecting google-cloud-vision\n",
            "  Downloading google_cloud_vision-3.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.29.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision) (2.43.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision) (1.27.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision) (5.29.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.4)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-cloud-vision) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2026.1.4)\n",
            "Downloading google_cloud_vision-3.12.0-py3-none-any.whl (538 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-vision\n",
            "Successfully installed google-cloud-vision-3.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "f91a7c0608ff46f0a01aff8b87b01b2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.29.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.188.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.27.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.1)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.3.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Google APIs\n",
        "For the group project, or just in general, rather than running a code locally on your system, it could be beneficial to have an external server run the code and provide the results to you. For this, APIs are very beneficial as they allow you to take advantage of complex code with only a single call over the internet. Google has a variety of models available through API calls including Gemini, Text to Speech, Vision Tools, Text Sentiment Analysis, etc., that could be very useful. This will provide a guide to setting up your account, enabling some APIs that could be useful for the group project, and providing some exposure on how to use them.\n",
        "\n",
        "## Setting Up Google Cloud\n",
        "\n",
        "1. Go to https://console.cloud.google.com/, this is the dashboard where you can control everything with respect to Google's APIs.\n",
        "2. Sign in with your **personal** account. Stanford accounts will not work.\n",
        "3. Set up your billing information with Google. Don't worry, we have credits that should cover all costs, but this is still required before they allow you to proceed.\n",
        "4. Open the Project Selector. Look for the project dropdown near the top-left of the Cloud Console. It might say “Select a project” if you have none yet.\n",
        "Click on the dropdown to expand the project selector.\n",
        "5. Fill out the required information and create the project. Note you do not need to specify an organization or change anything from the defaults unless you specifically want to.\n",
        "6. Once the project is created, you should see it in the top left of the screen.\n",
        "7. The next step will be to enable any APIs that you would want. In the navigation menu on the left of the screen, go to \"APIs and Services\" and then \"API Library\".\n",
        "8. Take a look at the provided APIs that Google offers. The APIs in the \"Machine Learning\" category are likely to be helpful for you. For each API that you are interested in, click on it and then click \"Enable\". This will enable it for the Project you created earlier.\n",
        "9. Enable \"Gemini API\", \"Cloud Speech-to-Text API\", and the \"Cloud Vision API\". These will all be needed for this homework.\n",
        "\n",
        "## Applying for Google Credits\n",
        "1. Go to the [Student Coupon Retrieval Link](https://gcp.secure.force.com/GCPEDU?cid=7glw7pjVf3GwBIfyRm8cja%2BefkuriDHDSLm9Vq5NKDT9fDA2dCLJQ%2F5IJRl%2BwZwe/) and fill out your name and university email.\n",
        "2. You will be asked to verify your email, and then the coupon will be delivered to your Stanford email.\n",
        "3. Click the link to redeem the code, this will open up the Google Cloud console. Type in the coupon code provided in the email and redeem the coupon to your account.\n",
        "4. You're done!\n",
        "\n",
        "## Using the APIs\n",
        "Now that the APIs are enabled, you'll want to use them. When you make a call via code, Google needs to know who is making the call. This is done one of two ways: an API key, or with the Gcloud App.\n",
        "\n",
        "### Running Code on Personal Computer\n",
        "The easiest way to run the APIs on your personal machine will be with the Google Cloud CLI\n",
        "1. Install the [gcloud CLI](https://cloud.google.com/sdk/docs/install#windows) for the platform of your choice\n",
        "2. In your terminal, run\n",
        "```\n",
        "gcloud init\n",
        "```\n",
        "3. Then in your local environment, for example in VSCode, run\n",
        "```\n",
        "gcloud auth login\n",
        "```\n",
        "This logs you in to the local environment and will allow you to use the APIs without explicitly setting keys.\n",
        "\n",
        "This works for all APIs **EXCEPT** Gemini, Gemini requires an API key to be configured for it.\n",
        "\n",
        "### Generating an API key for Gemini\n",
        "1. For Gemini, an API key can be generated by going to \"APIs & Services\" in the navigation menu.\n",
        "2. Click on \"Credentials\" then \"Create Credentials\" then \"API Key\"\n",
        "3. This API key can then be used anywhere for Gemini. Be careful when sharing code with this key to avoid unwanted charges.\n",
        "\n",
        "### Creating a JSON Key\n",
        "For other situations, such as this homework or running code on other machines, a JSON key can be used to store your credentials that can easily be used anywhere.\n",
        "1. Go to \"IAM & Admin\" from the navigation menu\n",
        "2. Click on \"Service Accounts\" in the dropdown menu.\n",
        "3. Create a service account. Fill out the details for the account.\n",
        "4. Assign a role for the account. The easiest option would be \"Owner\". This will give it all the permissions that you would expect.\n",
        "5. Click \"Done\". This will create the service account.\n",
        "6. Click on the service account. This should take you to information about the service account.\n",
        "7. Click \"Keys\" then \"Add Key\" and then \"Create New Key\".\n",
        "8. Create a new JSON Key. It will be saved to your local device.\n",
        "9. You can use this key by setting the path to it to the \"GOOGLE_APPLICATION_CREDENTIALS\" environment variable. See below for an example implementation.\n",
        "\n",
        "This appears to work for all APIs **INCLUDING** Gemini\n",
        "\n"
      ],
      "metadata": {
        "id": "iNGSEzRNBU2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-iMUMw0LpxP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf30c3b8-a263-4c5f-8d89-60cf481b6b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mpl_toolkits.mplot3d.axes3d as p3\n",
        "import numpy as np\n",
        "from matplotlib import animation\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.integrate import solve_ivp\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import wave\n",
        "from google.cloud import speech_v1p1beta1 as speech\n",
        "from google.colab import files\n",
        "import google.generativeai as genai\n",
        "import io\n",
        "from google.cloud import vision\n",
        "from PIL import Image as PILImage, ImageDraw, ImageFont\n",
        "from IPython.display import display\n",
        "\n",
        "# %matplotlib notebook #keep commented if using Google Colab, otherwise uncomment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FGgge9RpxP3"
      },
      "source": [
        "## Readme:\n",
        "This homework leverages an accompanying dataset. Please find it in canvas, and put it in the same level as this notebook. If you are using google colab there are a few ways to accomplish this [found on this page](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=jRQ5_yMcqJiV), mounting the drive locally, for example:\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ITMEsDSFtIsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f141368e-9e4d-4cb1-f8ca-4cab69bc9478"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running the above, in the side panel on the left, open the folder, one symbol at the top allows you to refresh the folder, open your drive folder and navigate to where you've put the data folder, at the data folder, hit the options button and select \"Copy Path\" and paste that string below:"
      ],
      "metadata": {
        "id": "aSHkSfw8tKtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/Colab Notebooks/HW2_Data'\n",
        "# data_path = 'data' #run this if you are running this locally and the data folder is at the same directory as the notebook\n",
        "\n",
        "# Path to where you uploaded the API key:\n",
        "json_key_path = \"/content/drive/MyDrive/Colab Notebooks/HW2_Data/cs339rhw2-f89bd3711893.json\" #this is an example, update with your path following instructions above\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = json_key_path"
      ],
      "metadata": {
        "id": "MyZoN3c4tPLI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Speech To Text\n",
        "\n",
        "An important part of the group project will be to convert speech into text. The user will be communicating to the Locobot via speech. However, raw audio is hard for computers to interpret. To get around this, speech to text models have been developed to understand and transcribe speech. Google offers an API to do this that allows access to these models wth a single API call.\n",
        "\n",
        "In this problem, you will fill in a function to call the Speech to Text API and have it return the transcribed text."
      ],
      "metadata": {
        "id": "_N_ppLU3Bj51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechTranscriber:\n",
        "    def __init__(self, language_code='en-US', sample_rate=16000):\n",
        "        \"\"\"\n",
        "        Initialize a SpeechTranscriber instance.\n",
        "\n",
        "        :param language_code: The language code for transcription, e.g., 'en-US'.\n",
        "        :param sample_rate: The sample rate (Hertz) of the audio file, default is 16000.\n",
        "        \"\"\"\n",
        "        self.language_code = language_code\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "        # Create the Speech client once\n",
        "        self.client = speech.SpeechClient()\n",
        "\n",
        "    def transcribe_audio(self, audio_content):\n",
        "        \"\"\"\n",
        "        Uses Google Cloud Speech-to-Text to transcribe the given audio content (bytes).\n",
        "        Returns the transcription as a string.\n",
        "\n",
        "        :param audio_content: The raw bytes of the audio file to be transcribed.\n",
        "        :return: A string of the combined transcription.\n",
        "        \"\"\"\n",
        "        audio = speech.RecognitionAudio(content=audio_content)\n",
        "\n",
        "        config = speech.RecognitionConfig(\n",
        "            sample_rate_hertz=self.sample_rate,\n",
        "            language_code=self.language_code,\n",
        "            enable_automatic_punctuation=True,\n",
        "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "        )\n",
        "        # Step 1: Perform the transcription\n",
        "        response = self.client.recognize(config=config, audio=audio)\n",
        "\n",
        "        # Step 2: Extract the transcription. Hint: we only want the first result\n",
        "        if not response.results:\n",
        "            return \"\"\n",
        "\n",
        "        first_result = response.results[0]\n",
        "        transcript = \" \".join(\n",
        "            alt.transcript.strip() for alt in first_result.alternatives if alt.transcript\n",
        "        ).strip()\n",
        "\n",
        "        #Step 3: Return the transcript\n",
        "        return transcript\n"
      ],
      "metadata": {
        "id": "M5rSanGsBnXr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test your functions with the following code\n",
        "tts = SpeechTranscriber()\n",
        "\n",
        "#Transcribe the first audio file. You should see \"I love collaborative robotics.\"\n",
        "audio_content1 = data_path + \"/API/recorded_audio1.wav\"\n",
        "with open(audio_content1, \"rb\") as audio_file:\n",
        "    audio_content1 = audio_file.read()\n",
        "transcription = tts.transcribe_audio(audio_content1)\n",
        "print(\"Transcription:\\n\", transcription)\n",
        "\n",
        "#Transcribe the second audio file\n",
        "audio_content2 = data_path + \"/API/recorded_audio2.wav\"\n",
        "with open(audio_content2, \"rb\") as audio_file:\n",
        "    audio_content2 = audio_file.read()\n",
        "transcription = tts.transcribe_audio(audio_content2)\n",
        "print(\"Transcription:\\n\", transcription)"
      ],
      "metadata": {
        "id": "6L8d3fWTBob3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4ad1bb-09fd-443d-d8e3-f2953bd02296"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:\n",
            " I love collaborative robotics.\n",
            "Transcription:\n",
            " These apis make speech to text so easy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: LLMs / VLMs\n",
        "LLMs (Large Language Models) and VLMs (Vision Language Models) are very useful in a large variety of situations. They generate text in response to a prompt consisting of text, an image or a combination of both. This makes them good for answering questions, sequential reasoning, pattern recognition, etc.\n",
        "\n",
        "LLMs can also recognize critical elements in text and respond in a carefully formatted way that the user specifies. This can be done by promp engineering, where the LLM is given context as to what the user wants prior to responding to the text. In this question, you will work with Gemini and explore its capabilities with text and images.\n",
        "\n",
        "You will first work to fill out the generate_content function so that Gemini will either respond to a query with and without a prompt influencing its response. Then you will fill out generate_from_image, so that Gemnini will respond to images as well."
      ],
      "metadata": {
        "id": "IC3emiVkBtx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiClass:\n",
        "    def __init__(self, prompt=None):\n",
        "        \"\"\"\n",
        "        Constructor for the GeminiClass.\n",
        "        Initializes the generative model and sets a default prompt.\n",
        "\n",
        "        Parameters:\n",
        "        - prompt (optional): A string to set as the default prompt. Defaults to an empty string if not provided.\n",
        "        \"\"\"\n",
        "        self.gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        if prompt:\n",
        "            self.prompt = prompt\n",
        "        else:\n",
        "            self.prompt = \"\"\n",
        "\n",
        "    def set_prompt(self, prompt):\n",
        "        \"\"\"\n",
        "        Sets or updates the prompt for the GeminiClass.\n",
        "\n",
        "        Parameters:\n",
        "        - prompt: A string that will serve as the base prompt for content generation.\n",
        "        \"\"\"\n",
        "        #Add code to set self.prompt to the provided prompt\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate_content(self, text, usePrompt):\n",
        "        \"\"\"\n",
        "        Generates content using the generative model.\n",
        "\n",
        "        Parameters:\n",
        "        - text: A string input to provide to the model for content generation.\n",
        "        - usePrompt: A boolean indicating whether to prepend the class's prompt to the input text.\n",
        "\n",
        "        Returns:\n",
        "        - The generated content as a string.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "        #Add code to generate content from text. The usePrompt boolean should determine if the prompt is used or not.\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate_from_image(self, image_bytes, textInput):\n",
        "        \"\"\"\n",
        "        Generates content based on an image and optional text input.\n",
        "\n",
        "        Parameters:\n",
        "        - image_bytes: The binary data of the image.\n",
        "        - textInput: A string input to accompany the image for content generation.\n",
        "\n",
        "        Returns:\n",
        "        - The generated content as a string.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "        # Add code that allows for the image and the text input to be passed to Gemini and return Gemini's response.\n",
        "\n",
        "        #Hint: Gemini can accept multiple inputs in the form of a list.\n",
        "\n",
        "        return\n"
      ],
      "metadata": {
        "id": "rIKnE7YRBwmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use this to test out the API\n",
        "gemini = GeminiClass()\n",
        "# Gemini should give an in depth explanation\n",
        "print(gemini.generate_content('Can you give me some information on how LLMs and VLMs work?',False))\n",
        "\n",
        "#Gemini should give you an explanation that is only 2 sentences long\n",
        "gemini.set_prompt(\"Please limit your response to 2 sentences.\")\n",
        "print(gemini.generate_content('Can you give me some information on how LLMs and VLMs work?',True))\n",
        "\n",
        "#Gemini should describe the picture to you\n",
        "image = data_path + '/API/image1.jpg'\n",
        "image = Image.open(image)\n",
        "display(image)\n",
        "print(gemini.generate_from_image(image, 'What do you see in this picture?'))"
      ],
      "metadata": {
        "id": "H1ixu-TIBxkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3: Vision\n",
        "While Gemini is quite good at recognizing what is in an image, it struggles to give specific values for locations of objects. Google offers a Cloud Vision API that detects objects in an image and can return the pixel locations of the object. This is important for robotics applications if you want a target position based on an image.\n",
        "\n",
        "In this you will fill out two functions: find_center and annotate_image. In annotate image, you will find all objects in the image, draw bounding boxes around them, and return the image. In find_center, you will return the center position of a specific object."
      ],
      "metadata": {
        "id": "HBob8HPVB1kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionObjectDetector:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Vision client once during object creation.\n",
        "        \"\"\"\n",
        "        self.client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    def find_center(self, image_bytes, object_name):\n",
        "        \"\"\"\n",
        "        Finds the center of an object (e.g., \"pineapple\") in the provided image bytes.\n",
        "\n",
        "        :param image_bytes: The raw bytes of the image.\n",
        "        :param object_name: The target object name to search for (case-insensitive).\n",
        "        :return: Tuple (pixel_x, pixel_y) of the object's approximate center, or None if not found.\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "        # Step 1: Create the Vision Image object from bytes\n",
        "\n",
        "        # Step 2: Send the image to the API for object localization\n",
        "\n",
        "        # Step 3: Extract localized object annotations\n",
        "\n",
        "        # Step 4: Search for the specified object. Hint: Objects returns all detected objects\n",
        "\n",
        "        # Step 5: Once the object is found, determine the position from the bounding box. Hint: obj.bounding_poly returns the bounding box\n",
        "\n",
        "        # Step 6: Find the center from the corners of the bounding box\n",
        "\n",
        "        # Step 7: Return the center in pixel coordinates. Hint: The position of the bounding box is normalized so you will need to convert it back into the dimensions of the image\n",
        "\n",
        "        return\n",
        "\n",
        "    def annotate_image(self, image_bytes):\n",
        "        \"\"\"\n",
        "        Detects all objects in the image and returns a PIL Image with\n",
        "        bounding boxes and labels drawn for each detected object.\n",
        "\n",
        "        :param image_bytes: The raw bytes of the image.\n",
        "        :return: A PIL Image object annotated with bounding boxes and labels.\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "        # Step 1: Create the Vision Image object from bytes\n",
        "\n",
        "        # Step 2: Send the image to the API for object localization\n",
        "\n",
        "        # Step 3: Extract localized object annotations\n",
        "\n",
        "        # Step 4: Open the image via PIL for drawing\n",
        "\n",
        "        # Step 5: Iterate through all the objects and draw the bounding boxes on the image.\n",
        "        # Hint: draw.polygon allows you to draw based on pixel coordinates\n",
        "\n",
        "        #Step 6: Return the annotated image\n",
        "\n",
        "        return\n",
        "\n"
      ],
      "metadata": {
        "id": "IoWcbBMxB45y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use this to test your code. For each picture it should return coordinates of the object and an annotated image\n",
        "\n",
        "#First file\n",
        "image_content1 = data_path + \"/API/image1.jpg\"\n",
        "with open(image_content1, \"rb\") as image_file:\n",
        "    image_content1 = image_file.read()\n",
        "detector = VisionObjectDetector()\n",
        "center_coordinates = detector.find_center(image_content1, 'pineapple')\n",
        "if center_coordinates:\n",
        "    print(\"Center in pixel coordinates:\", center_coordinates)\n",
        "annotated_image = detector.annotate_image(image_content1)\n",
        "display(annotated_image)\n",
        "\n",
        "#Second file\n",
        "image_content2 = data_path + \"/API/image2.jpg\"\n",
        "with open(image_content2, \"rb\") as image_file:\n",
        "    image_content2 = image_file.read()\n",
        "detector = VisionObjectDetector()\n",
        "center_coordinates = detector.find_center(image_content2, 'dog')\n",
        "if center_coordinates:\n",
        "    print(\"Center in pixel coordinates:\", center_coordinates)\n",
        "annotated_image = detector.annotate_image(image_content2)\n",
        "display(annotated_image)"
      ],
      "metadata": {
        "id": "eG4ZT5CDB7EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4: Combined Example\n",
        "Now you have worked with the three APIs individually, but where the APIs really shine is when they are used together. By utilizing their strengths together, you can build a pipeline.\n",
        "\n",
        "For example in the group project, you might want to listen for what a person wants and then find where the object is in the scene. This requires recording the person's request, converting it to text, and then parsing the request to find what the object is. Once the obect is known, you would want to find its position in the scene to provide a target for the robot to navigate to.\n",
        "\n",
        "In this, you will utilize the previous 3 APIs to develop a pipeline that can take an audio file and an image. The auido file will contain a statement of what the user wants i.e. \"Bring me the banana\", and the function should return the pixel coordinates of the banana in the image.\n"
      ],
      "metadata": {
        "id": "JF8o_ERRB9mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedPipeline:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor for the CombinedPipeline class.\n",
        "        Initializes instances of the SpeechTranscriber, VisionObjectDetector,\n",
        "        and GeminiClass for processing audio, image, and text data.\n",
        "        \"\"\"\n",
        "        self.tts = SpeechTranscriber()\n",
        "        self.detector = VisionObjectDetector()\n",
        "        # Initializes the GeminiClass with a prompt to extract the object from the voice transcript.\n",
        "        self.gemini = GeminiClass(prompt=\"In the given voice transcript, identify what the object is that the user wants. Return only the object in lowercase and do not include any whitespaces, punctuation, or new lines. Here is the voice transcript: \")\n",
        "\n",
        "        )\n",
        "\n",
        "    def combined_pipeline(self, audio, image):\n",
        "        \"\"\"\n",
        "        Processes audio and image inputs to determine the center coordinates of the object\n",
        "        specified in the audio.\n",
        "\n",
        "        Parameters:\n",
        "        - audio: The audio input to be transcribed into text.\n",
        "        - image: The image input in which the object should be detected.\n",
        "\n",
        "        Returns:\n",
        "        - center_coordinates: A tuple representing the (x, y) coordinates of the object's center,\n",
        "          or None if the object is not found.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "        # Hint: Even if Gemini responds with a single word, a new line character is always added at the end of the response.\n",
        "        # Make sure to strip any symbols from the response before passing it to the vision detector.\n",
        "\n",
        "        return\n"
      ],
      "metadata": {
        "id": "wM3LkwhHB__7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use this to test out the API. The coordinates of the basketball should be returned\n",
        "\n",
        "audio_content3 = data_path + \"/API/recorded_audio3.wav\"\n",
        "with open(audio_content3, \"rb\") as audio_file:\n",
        "    audio_content3 = audio_file.read()\n",
        "image_content3 = data_path + \"/API/image3.jpg\"\n",
        "with open(image_content3, \"rb\") as image_file:\n",
        "    image_content3 = image_file.read()\n",
        "\n",
        "pipeline = CombinedPipeline()\n",
        "center_coordinates = pipeline.combined_pipeline(audio_content3, image_content3)\n",
        "annotated_image = detector.annotate_image(image_content3)\n",
        "display(annotated_image)\n",
        "print(\"Center in pixel coordinates:\", center_coordinates)"
      ],
      "metadata": {
        "id": "eXzVPescCBOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5: Feature Detection"
      ],
      "metadata": {
        "id": "qmgwF4kRCH7u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzibEGSvpxP4"
      },
      "source": [
        "# Problem 5.1: Vision Color Detection\n",
        "\n",
        "It is often helpful to use every feature at your disposal to extract information and context from a scene. Useful for the project is color detection of blocks in the scene. In this problem, you will generate masks for each color that will indicate which pixels are the particular color. This will require some hand-tuning to get sharp images of the specified colors.\n",
        "\n",
        "You may find it particuarly helpful to filter the colors using the Hue, Saturation, Value space (HSV). Given a color specification in the set ['r','g','b','y'] (red,green,blue,yellow), your script must return an image with the blocks colors indicated (either show them as white and the rest of the pixels as black, or show the true color). You may use all opencv/python-base libraries at your disposal.\n",
        "\n",
        "\n",
        "HSV space:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Hsl-hsv_models.svg/1200px-Hsl-hsv_models.svg.png\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C94ggHO3pxP4"
      },
      "outputs": [],
      "source": [
        "def student_function_1(color_img, color_mask='r'):\n",
        "    \"\"\"Returns the black and white image with a color-mask for the specified color (white or the color where the color is, black everywhere else)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    color_img : np.ndarray\n",
        "        Raw input image of colored blocks on a table\n",
        "    color_mask : string\n",
        "        String indicating which color to draw the mask on; options 'r', 'g','b','y' (red, green, blue, yellow)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mask_img : np.ndarray\n",
        "        Image with just the selected color shown (either with true color or white mask)\n",
        "    \"\"\"\n",
        "    #Hint: one way would be to define threshold in HSV, leverage that to make a mask?\n",
        "\n",
        "    raise NotImplementedError\n",
        "    #Step 1: Convert to HSV space; OpenCV uses - H: 0-179, S: 0-255, V: 0-255\n",
        "\n",
        "    #Step 2: prep the mask\n",
        "\n",
        "    #Step 3: Apply the mask; black region in the mask is 0, so when multiplied with original image removes all non-selected color\n",
        "\n",
        "    return mask_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz78ffJWpxP5"
      },
      "source": [
        "### Code Verification\n",
        "Please run the following blocks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imOS3FVMpxP5"
      },
      "outputs": [],
      "source": [
        "c1_img = cv2.cvtColor(cv2.imread(data_path + \"/color/c1.jpg\", cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)#default cv2 read-in is bgr (blue, green, red) but cameras often use RGB convention, so we simply reorder to get it right here.\n",
        "c1_red_img = student_function_1(c1_img,color_mask='r')\n",
        "c1_green_img = student_function_1(c1_img,color_mask='g')\n",
        "c1_blue_img = student_function_1(c1_img,color_mask='b')\n",
        "c1_yellow_img = student_function_1(c1_img,color_mask='y')\n",
        "plt.figure()\n",
        "plt.subplot(221)\n",
        "plt.imshow(c1_red_img)\n",
        "plt.subplot(222)\n",
        "plt.imshow(c1_green_img)\n",
        "plt.subplot(223)\n",
        "plt.imshow(c1_blue_img)\n",
        "plt.subplot(224)\n",
        "plt.imshow(c1_yellow_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fssdUIMnpxP6"
      },
      "outputs": [],
      "source": [
        "c2_img = cv2.cvtColor(cv2.imread(data_path + \"/color/c2.jpg\", cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
        "c2_red_img = student_function_1(c2_img,color_mask='r')\n",
        "c2_green_img = student_function_1(c2_img,color_mask='g')\n",
        "c2_blue_img = student_function_1(c2_img,color_mask='b')\n",
        "c2_yellow_img = student_function_1(c2_img,color_mask='y')\n",
        "plt.figure()\n",
        "plt.subplot(221)\n",
        "plt.imshow(c2_red_img)\n",
        "plt.subplot(222)\n",
        "plt.imshow(c2_green_img)\n",
        "plt.subplot(223)\n",
        "plt.imshow(c2_blue_img)\n",
        "plt.subplot(224)\n",
        "plt.imshow(c2_yellow_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvVJdlpIpxP6"
      },
      "outputs": [],
      "source": [
        "c3_img = cv2.cvtColor(cv2.imread(data_path + \"/color/c3.jpg\", cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
        "c3_red_img = student_function_1(c3_img,color_mask='r')\n",
        "c3_green_img = student_function_1(c3_img,color_mask='g')\n",
        "c3_blue_img = student_function_1(c3_img,color_mask='b')\n",
        "c3_yellow_img = student_function_1(c3_img,color_mask='y')\n",
        "plt.figure()\n",
        "plt.subplot(221)\n",
        "plt.imshow(c3_red_img)\n",
        "plt.subplot(222)\n",
        "plt.imshow(c3_green_img)\n",
        "plt.subplot(223)\n",
        "plt.imshow(c3_blue_img)\n",
        "plt.subplot(224)\n",
        "plt.imshow(c3_yellow_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d32-YpwfpxP6"
      },
      "source": [
        "Often color is not the only nor best feature to detect in a scene. Alternatively edges and corners serve as better features to detect. In the following functions you will leverage opencv functions to find edges and corners (good features to track between frames)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gumY70IIpxP7"
      },
      "source": [
        "## Problem 5.2: Edge Detection\n",
        "For this problem, you will want to leverage the *Canny Edge detector* to track edges in the frame (arguments will be directly useful for its functionality, see documentation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3TD1StopxP7"
      },
      "outputs": [],
      "source": [
        "def student_function_2(img_input, threshold1=100, threshold2=200):\n",
        "    \"\"\" this script takes in an image and returns the edges using the Canny Edge detector algorithm\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_input : np.ndarray\n",
        "        Raw input image of environment\n",
        "    threshold1 : float\n",
        "        Float value for lower threshold of canny hysteresis procedure\n",
        "    threshold2 : float\n",
        "        Float value for upper threshold of canny hysteresis procedure\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Canny_edges : np.ndarray\n",
        "        Image with canny edges\n",
        "    \"\"\"\n",
        "    #Hint: Documentation exists for the cv2 canny function...\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return edge_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G78RoeJQpxP7"
      },
      "source": [
        "### 5.2 Evaluation\n",
        "Please run the following block for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKICReQjpxP7"
      },
      "outputs": [],
      "source": [
        "#load image for edge detection\n",
        "feature_img = cv2.cvtColor(cv2.imread(data_path + \"/feature_pose/pose1.jpg\", cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)#default cv2 read-in is bgr (blue, green, red) but cameras often use RGB convention, so we simply reorder to get it right here.\n",
        "#send to Student script for edge detection\n",
        "edge_img = student_function_2(feature_img,threshold1=10, threshold2=60)\n",
        "#display img\n",
        "plt.figure()\n",
        "plt.subplot(211)\n",
        "plt.imshow(feature_img)\n",
        "plt.subplot(212)\n",
        "plt.imshow(edge_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCiCu0BFpxP8"
      },
      "source": [
        "## Problem 5.3: Tracking 'Good Features'\n",
        "Lines can be useful, but often to track an object or set of objects, its important to track corner features. Opencv makes it easy to leverage algorithms like *Good Features to Track* which rely on algorithms like *Shi-Tomasi Corner Detector*. In this function, you will leverage the *Good Features to Track* algorithm to track corners in the scene (useful towards tracking rigid bodies or fiducials (e.g. apriltags))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60_Xa1hKpxP8"
      },
      "outputs": [],
      "source": [
        "def student_function_3(img_input, maxCorners=100, qualityLevel=200,min_distance=10):\n",
        "    \"\"\" This function takes in an image and detects corners\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_input : np.ndarray\n",
        "        Raw input image of environment\n",
        "    maxCorners : float\n",
        "        Maximum number of corners to return. If there are more corners than are\n",
        "        found, the strongest of them is returned.\n",
        "    qualityLevel : float\n",
        "        Parameter characterizing the minimal accepted quality of image corners.\n",
        "        The parameter value is multiplied by the best corner quality measure,\n",
        "        which is the minimal eigenvalue (see cornerMinEigenVal ) or the Harris\n",
        "        function response (see cornerHarris ). The corners with the quality\n",
        "        measure less than the product are rejected. For example, if the best\n",
        "        corner has the quality measure = 1500, and the qualityLevel=0.01 ,\n",
        "        then all the corners with the quality measure less than 15 are rejected\n",
        "    min_distance : int\n",
        "        Minimum possible Euclidean distance between the returned corners\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Canny_edges : np.ndarray\n",
        "        Gray scale image with canny edges\n",
        "    \"\"\"\n",
        "    #Hint: Documentation exists for the cv2 Shi-Tomasi Corner Detector & Good Features to Track...\n",
        "    #Hint: copying an image: newImage = myImage.copy()\n",
        "    raise NotImplementedError\n",
        "\n",
        "    #Step 1: Gray scale the original image\n",
        "\n",
        "    #Step 2: Extract corners from the image\n",
        "\n",
        "    #Step 3: redraw them (as circles) on a copy of the image to return\n",
        "\n",
        "    return img_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFfEp_BGpxP8"
      },
      "source": [
        "## 5.3 Evaluation\n",
        "Please run the following block for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYP0r_qHpxP8"
      },
      "outputs": [],
      "source": [
        "#load image for feature detection\n",
        "raw_img = cv2.cvtColor(cv2.imread(data_path+\"/feature_pose/pose1.jpg\", cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)#default cv2 read-in is bgr (blue, green, red) but cameras often use RGB convention, so we simply reorder to get it right here.\n",
        "#send to Student script for feature detection\n",
        "good_features_img = student_function_3(raw_img, maxCorners=500, qualityLevel=0.01,min_distance=10)\n",
        "#display img\n",
        "plt.figure()\n",
        "plt.subplot(211)\n",
        "plt.imshow(raw_img)\n",
        "plt.subplot(212)\n",
        "plt.imshow(good_features_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNC67LsepxP9"
      },
      "source": [
        "## Problem 6: Perspective-n-Point (PnP)\n",
        "\n",
        "Given an ability to track good features in our scene, if we have some prior knoweldge about rigid bodies in the scene then we can leverage that to track the relative pose between the camera and the tracked object. In the following script, you will perform pose tracking of an [AprilTag](https://people.csail.mit.edu/kaess/apriltags/) *however* **you must either use OpenCV to perform perspective-n-point or generate this algorithm yourself (you may not use Apriltag base code to solve this problem)**.\n",
        "\n",
        "Points on the tags are indexed as follows:\n",
        "\n",
        "<div>\n",
        "<img src=\"https://berndpfrommer.github.io/tagslam_web/media/tag_corners.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "We will denote the points on the tag (0:3) (in the csv, the index name goes from (1:4)) in the frame of the tag (center reference frame) as $(w_{i,x},w_{i,y})$ and in the image plane, we will use pixels $(u_{i,x},u_{i,y})$. The goal is to determine the pose of the tags reference frame relative to the camera's reference frame.\n",
        "\n",
        "The calibration matrix of the camera is $$K = \\begin{bmatrix}\n",
        "915.1482543945312 & 0.0 & 633.98828125 \\\\\n",
        "0.0 & 915.0738525390625 & 363.17529296875 \\\\\n",
        " 0.0 & 0.0 & 1.0\n",
        "\\end{bmatrix}  = \\begin{bmatrix}\n",
        "f_x & 0 & c_x \\\\\n",
        "0 & f_y & c_y \\\\\n",
        " 0 & 0 & 1.0\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "The camera frames follows the convention shown here for either of the fish-eye lenses (**Camera used for this data collection did not have noticable distortion**):\n",
        "<div>\n",
        "<img src=\"https://user-images.githubusercontent.com/6543766/64158631-20303a00-ce39-11e9-9c63-fe0baa8135ff.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Camera Model:\n",
        "$$\\lambda \\begin{bmatrix}u \\\\ v \\\\ 1 \\end{bmatrix}_{3\\times 1} = \\underbrace{ \\underbrace{\\begin{bmatrix}\n",
        "f_x & 0 & c_x \\\\\n",
        "0 & f_y & c_y \\\\\n",
        " 0 & 0 & 1.0\n",
        "\\end{bmatrix}}_{K_{3\\times 3}}\n",
        "\\begin{bmatrix}\n",
        "r_{11} & r_{12} & r_{13} & t_x \\\\\n",
        "r_{21} & r_{22} & r_{23} & t_y \\\\\n",
        "r_{31} & r_{32} & r_{33} & t_z \\\\\n",
        "\\end{bmatrix}}_{P_{3\\times 4}} \\underbrace{\\begin{bmatrix} w_X \\\\ w_Y \\\\ w_Z \\\\ 1 \\end{bmatrix}}_{\\tilde{X}_{4\\times 1}}$$\n",
        "By leveraging the crossproduct relation $[u]_{[\\times]} v = u \\times v$ (vector cross product with itself is zero) we have the relation:\n",
        "$$\\begin{bmatrix}u \\\\ v \\\\ 1 \\end{bmatrix}_{[\\times]} P \\tilde{X} = 0_{3 \\times 1} $$\n",
        "\n",
        "So by reorganizing with the rows of $P$ being represented by\n",
        "$$P = \\begin{bmatrix} P_{1,1 \\times 4} \\\\ P_{2,1 \\times 4} \\\\ P_{3,1\\times 4} \\end{bmatrix}$$ then we have the relation:\n",
        "\n",
        "$$\\underbrace{\\begin{bmatrix}\n",
        "0_{1\\times 4} & -\\tilde{X}^T & v \\tilde{X}^T\\\\\n",
        "\\tilde{X}^T & 0_{1\\times 4} & -u \\tilde{X}^T \\\\\n",
        "- v \\tilde{X}^T & u \\tilde{X}^T & 0_{1\\times 4}\n",
        "\\end{bmatrix}_{3\\times 12}}_{A_i} \\begin{bmatrix}P_1^T \\\\ P_2^T \\\\ P_3^T \\end{bmatrix} = 0_{3\\times 1}$$\n",
        "\n",
        "for the i'th point, then for 4 points we have:\n",
        "$$\\underbrace{\\begin{bmatrix}A_1 \\\\A_2 \\\\A_3 \\\\A_4  \\end{bmatrix}_{12 \\times 12}}_{\\tilde{A}} \\begin{bmatrix}P_1^T \\\\ P_2^T \\\\ P_3^T \\end{bmatrix}_{12 \\times 1} = 0_{12\\times 1}$$\n",
        "\n",
        "## Steps:\n",
        "\n",
        "There are two ways to solve this problem, the easier (and full points) is to use the opencv Perspective-n-Point function `cv2.solvePnP()` [documentation](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d). Alternatively, for your own development you may choose to reimplement the algorithm yourself (no extra points, just self-betterment and kudos!), if you decide to do that, a general step-by-step suggested algorithm is presented below. **Solutions will be provided for the opencv function only**\n",
        "\n",
        "### If you decide to implement the full algorithm yourself (as opposed to opencv PnP function), some helpful steps include:\n",
        "#### Step 1: Generate $\\tilde{A}$ Matrix\n",
        "Recall $\\underbrace{\\begin{bmatrix}\n",
        "0_{1\\times 4} & -\\tilde{X}^T & v \\tilde{X}^T\\\\\n",
        "\\tilde{X}^T & 0_{1\\times 4} & -u \\tilde{X}^T \\\\\n",
        "- v \\tilde{X}^T & u \\tilde{X}^T & 0_{1\\times 4}\n",
        "\\end{bmatrix}_{3\\times 12}}_{A_i}$\n",
        "\n",
        "#### Step 2: Use SVD on $\\tilde{A}$ to determine possible solutions of P\n",
        "Using the Right eigenvectors with Singular Value Decomposition (SVD), which are feasible solutions? (which would place the camera position in front of the camera (positive $p_z$)? Note, you may have to solve the other steps below to obtain $p_z$ to make sure its feasible.\n",
        "[Hint](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html):\n",
        "`np.linalg.svd`\n",
        "**SVD**\n",
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Singular_value_decomposition_visualisation.svg/440px-Singular_value_decomposition_visualisation.svg.png\" width=\"200\"/>\n",
        "</div>\n",
        "\n",
        "#### Step 3: Given a potential solution of P, extract the rotation and translation\n",
        "Recall $P = K[R,t]$, so given $K$ you can find $[R,t]$. But remember, R is an orthogonal, positive definite matrix $r_1 \\times r_2 = r_3$ ($r_i$ is column of $R$) and $det(R) = +1$.\n",
        "\n",
        "#### Step 4: convert Rotation matrix to quaternion then return position and quaternion is the function request\n",
        "Return $\\begin{bmatrix}p_x, p_y, p_z, q_x, q_y, q_z, q_w \\end{bmatrix}$. Check your first two answers against those given in the csv file, the rest are zeros you will have to populate.\n",
        "Hint: [scipy.spatial.transform.Rotation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcCqPudkpxP9"
      },
      "source": [
        "## Your Code Here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijn0txSKpxP9"
      },
      "outputs": [],
      "source": [
        "def quat_from_axis_angle(v):\n",
        "    \"\"\"\n",
        "    Helper function if you have an axis-angle vector and wish to find the quaternion\n",
        "    Params\n",
        "    ---\n",
        "    v: np.ndarray (3x1)\n",
        "        axis-angle representation of rotation\n",
        "    Returns\n",
        "    ---\n",
        "    q: list\n",
        "        list of quaternion elements\n",
        "    \"\"\"\n",
        "    angle = np.linalg.norm(np.matrix(v))\n",
        "    vect = v/angle\n",
        "    qw = np.cos(angle/2)\n",
        "    qx = vect.item(0)*np.sin(angle/2)\n",
        "    qy = vect.item(1)*np.sin(angle/2)\n",
        "    qz = vect.item(2)*np.sin(angle/2)\n",
        "    return [qx,qy,qz,qw]\n",
        "\n",
        "def student_function_4(world_xyz_pts,pixel_uv_pts,K_calib,dist=np.zeros((4,1))):\n",
        "    \"\"\"\n",
        "    This function takes in the world points, camera points, calibration matrix and returns the pose of the tag relative to the\n",
        "    camera frame\n",
        "    Parameters\n",
        "    ----------\n",
        "    pixel_uv_pts : np.ndarray\n",
        "        List of input pixel points of each corner of the tag [[p1x,p1y],[p2x,p2y],[p3x,p3y],[p4x,p4y]]\n",
        "        (using the notation from the demonstration figure, index from 0:3 yeilds [[p0x,p0y],[p1x,p1y],[p2x,p2y],[p3x,p3y]])\n",
        "    world_xyz_pts : np.ndarray\n",
        "        The tag has a constant size (and shape - flat), hence this array is constant: [['w1x', 'w1y', 'w1z'], ['w2x', 'w2y', 'w2z'], ['w3x', 'w3y', 'w3z'], ['w4x', 'w4y', 'w4z']]\n",
        "\n",
        "    K_calib : np.ndarray (3x3)\n",
        "        Parameter characterizing the minimal accepted quality of image corners.\n",
        "        The parameter value is multiplied by the best corner quality measure,\n",
        "        which is the minimal eigenvalue (see cornerMinEigenVal ) or the Harris\n",
        "        function response (see cornerHarris ). The corners with the quality\n",
        "        measure less than the product are rejected. For example, if the best\n",
        "        corner has the quality measure = 1500, and the qualityLevel=0.01 ,\n",
        "        then all the corners with the quality measure less than 15 are rejected\n",
        "    dist : np.ndarray (4x1)\n",
        "        Distortion params, if distortion is not significant these can be left as zeros (applicable for intel realsense camera used for this dataset)\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    pose : np.ndarray\n",
        "        pose of the tag in camera frame\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "    return pose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55IC3M9RpxP-"
      },
      "source": [
        "## Evaluation Prob 6\n",
        "Points associated with the images are saved in a CSV file, the ordering of the points are shown below. For the first two points, the poses of the tag in the camera frame are provided, your script must correctly fill in the rest.\n",
        "**Run the block below for evaluation, then upload the resulting csv with your notebook to Canvas**\n",
        "$$\\begin{bmatrix} ID & u_{c,x}& u_{c,y} & u_{1,x}& u_{1,y} & u_{2,x} & u_{2,y} & u_{3,x} &  u_{3,y} & u_{4,x} & u_{4,y} & w_{1,x} & w_{1,y} & w_{1,z} & w_{2,x} & w_{2,y} & w_{2,z} & w_{3,x} & w_{3,y} & w_{3,z} & w_{4,x} & w_{4,y} & w_{4,z} & p_x & p_y & p_z & q_x & q_y & q_z & q_w \\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFccK3bUpxP-"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "#key is to put the data folder at the same level as the notebook when running locally\n",
        "with open(data_path + '/feature_pose/feature_keyframes.csv', newline='') as csvfile:\n",
        "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "    feature_list = []\n",
        "    for row in spamreader:\n",
        "        feature_list.append(row[:])\n",
        "\n",
        "#convert csv of strings to numbers for appropriate rows/cols:\n",
        "for idx in range(1,len(feature_list)):\n",
        "    for jdx in range(1,len(feature_list[idx])):\n",
        "        feature_list[idx][jdx] = float(feature_list[idx][jdx])\n",
        "\n",
        "def extract_information(pr):\n",
        "    pixel_uv_pts = np.array([[pr[3],pr[4]],\n",
        "                              [pr[5],pr[6]],\n",
        "                              [pr[7],pr[8]],\n",
        "                              [pr[9],pr[10]]])\n",
        "    world_xyz_pts = np.array([[pr[11],pr[12],pr[13]],\n",
        "                             [pr[14],pr[15],pr[16]],\n",
        "                             [pr[17],pr[18],pr[19]],\n",
        "                             [pr[20],pr[21],pr[22]]])\n",
        "    return pixel_uv_pts,world_xyz_pts\n",
        "\n",
        "#make the calibration matrix K (comes from intelrealsense for given camera (provided in camera_info topic))\n",
        "calib_matrix = np.array([[915.1482543945312,0.0 , 633.98828125],\n",
        "                         [0.0 , 915.0738525390625 , 363.17529296875],\n",
        "                         [0.0 , 0.0 , 1.0]])\n",
        "\n",
        "predicted_pose_list = []\n",
        "for idx in range(7):\n",
        "    pose_row = feature_list[idx+1]\n",
        "    pixel_uv_pts,world_xyz_pts = extract_information(pose_row)\n",
        "    pose = student_function_4(world_xyz_pts,pixel_uv_pts,calib_matrix)\n",
        "    predicted_pose_list.append(pose)\n",
        "\n",
        "student_feature_list = np.copy(feature_list) #initalize with previous list\n",
        "for idx in range(7):\n",
        "    for jdx in range(7):\n",
        "        student_feature_list[idx+1][jdx+23] = predicted_pose_list[idx][jdx]\n",
        "\n",
        "#now save this to a new csv file to be saved for grading\n",
        "with open(data_path+'feature_keyframes_student.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
        "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for idx in range(8):\n",
        "        spamwriter.writerow(student_feature_list[idx].tolist())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}